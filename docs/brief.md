# Project Brief: Hewston
> Status: Draft 1 complete — 2025-09-22


> Mode: Interactive
> Generated by BMAD workflow (project-brief-template-v2)

## Introduction
This document is being created collaboratively in interactive mode. No prior research inputs were provided. We will iterate section by section and refine based on elicitation.

## Executive Summary (Draft 2)
- Product concept (1–2 sentences): Hewston is an autonomous AI day‑trading app for a single operator. The MVP delivers ultra‑fast, reproducible backtests with live‑like playback.
- Primary problem: Strategy iteration is slow, opaque, and hard to reproduce/compare across runs.
- Target scope: Single user; equities first (with future expansion to FX/crypto).
- Differentiator/value: Near‑instant backtests via Nautilus Trader on pre‑fetched Databento data, with time‑compressed playback (~1 year → ~60s) in a Vite/TS UI using TradingView Lightweight Charts.
- Modes & phasing: Backtesting (MVP now); Paper Trading and Live Trading later.
- Persistence & reproducibility: Each run stores a manifest (params, code hash, data snapshot) and outputs (metrics, equity curve, orders/fills) for re‑runs and audit.
- Platform choices: Python backend (FastAPI) + Nautilus; Vite + React + TypeScript + Tailwind CSS frontend; Makefile with `make start` to run both.


## Problem Statement (Draft 0)
- Current state and pain points:
  - Evaluating strategies is slow and manual; results are difficult to reproduce or compare across iterations.
  - Most tools emphasize batch backtests without live‑like playback and lack persistent run artifacts (params, code/data versions).
  - Data prep (1y trades + TBBO from Databento) adds friction: sourcing, normalization, calendaring, deterministic bar generation.
- Impact:
  - Fewer experiment cycles, delayed confidence to progress to paper/live modes.
  - Higher risk of overfitting or misinterpretation without standardized playback and core metrics.
  - Time lost to repeated data wrangling instead of strategy learning.
- Why existing solutions fall short:
  - Significant boilerplate per run; little support for time‑compressed replay; inconsistent capture of orders/fills/equity and metrics in a searchable, versioned store.
- Urgency:
  - A fast, reproducible backtesting loop is prerequisite to responsibly advancing to paper and live trading.
- Audience/scope: Single user (owner‑operator); equities first.
- Performance target: Time‑compressed playback ≈1 year of data → ≈60s.
- Reproducibility: Each run must persist params, random seed, code hash, data snapshot IDs, and outputs.
- MVP boundary: Backtesting only; paper and live trading explicitly deferred.
- Operability: `make start` runs backend (FastAPI) + frontend (Vite) for the backtesting dashboard.


## Proposed Solution (Draft 0)
- System overview: Python backend (FastAPI) orchestrates Nautilus Trader backtests on pre‑fetched Databento data; Vite/TypeScript frontend renders time‑compressed playback with TradingView Lightweight Charts.
- Data layer:
  - Source: Databento TRADES + TBBO for selected symbols and date ranges (multi‑year supported).
  - Ingestion job: `make data` downloads/caches raw DBN, derives 1m OHLCV + minute TBBO aggregates (bid_mean, ask_mean, spread_mean) using NASDAQ market calendar; TZ=America/New_York; stores Parquet + dataset manifest.
  - Layout:
    data/raw/databento/{SYMBOL}/{YEAR}/{product}.dbn
    data/derived/bars/{SYMBOL}/{YEAR}/1m.parquet
    data/derived/bars/{SYMBOL}/{YEAR}/manifest.json
- Backtest runner:
  - Inputs: StrategyConfig (id, params, seed), dataset id, interval='1m', date range.
  - Constraint (MVP): single‑symbol per run.
  - Execution: Nautilus runs via adapter that reads 1m bars; TBBO aggregates inform a simple slippage/fees model; computes core metrics; captures orders/fills/equity.
  - Outputs per run: metrics.json, equity.parquet, orders.parquet, fills.parquet, run‑manifest.json (params, code hash, data snapshot IDs, timestamps).
  - Persistence layout: `data/backtests/{run_id}/...` with stable `run_id` used by API/UI.
- Playback service (API):
  - POST `/backtests` {symbol, from, to, interval='1m', strategy_id='sma_crossover', params, speed=60, seed} → create run (async)
  - GET `/backtests` → list; GET `/backtests/{id}` → metadata + artifacts
  - GET `/backtests/{id}/stream?speed=60` → SSE time‑compressed frames with decimation
  - Target speed: ≈1 year → ≈60 seconds, adjustable; frame skipping when needed.
- Frontend (Vite/React/TS + Tailwind):
  - Views: Runs list; Run detail (chart + equity + orders/fills overlays); controls for play/pause/seek/speed; rerun button (pre‑filled from manifest).
- Strategy model:
  - MVP: one baseline strategy (e.g., simple crossover) to validate pipeline; StrategyRegistry to add more later; “agent per strategy” kept as future abstraction.
- Developer UX:
  - `make start` starts backend (Uvicorn) + frontend dev server.
  - `make data SYMBOL=AAPL YEAR=2023` builds/caches dataset.
  - `make backtest SYMBOL=AAPL FROM=2023-01-01 TO=2023-12-31 STRATEGY=baseline SPEED=60s` submits a run.
- Explicit non‑goals (MVP): paper trading, live trading, brokerage integration, multi‑user, portfolio/multi‑symbol optimization, advanced risk mgmt.
- Risk mitigations (MVP commitments):
  - Calendars/TZ: Pin NASDAQ calendar version; handle DST explicitly; manifest records calendar + corporate‑action data versions.
  - Slippage/fees: fill ≈ mid ± k·spread_mean + fixed bps; document limits of 1m bars vs sub‑minute tactics.
  - Manifests: include git hash and environment lock; reject runs lacking complete metadata.
  - Streaming: server decimation to ~30 FPS; SSE with chunking; client Web Worker parsing and backpressure (drop oldest) when needed.
  - Storage: Parquet partitioned by symbol/year; lightweight run catalog (JSON/SQLite) and retention policy.

- Alignment refinements:
  - Multi‑symbol design: API/manifest accept symbols:[..]; MVP enforces exactly one symbol to simplify playback; future allows multiple.
  - Corporate actions: Store unadjusted bars; manifest includes corp‑action source/version; apply adjustments only in analytics as needed.
  - Slippage/fees defaults: k=0.5×spread_mean; fees=1 bps (configurable in StrategyConfig and recorded in run manifest).
  - Metrics (MVP set): total return, CAGR, max drawdown, Sharpe/Sortino, hit rate, avg win/loss, turnover, slippage/fees share.
  - Catalog: Lightweight runs catalog (JSON/SQLite) for list/filter/search (symbol, date, strategy); datasets catalog for discoverability; API pagination + filters; reject runs missing required artifacts/metadata.


## Goals & Success Metrics (Refined)
### Business Objectives
- MVP delivers reproducible backtests with ~1y→~60s playback and persistent artifacts (manifest + metrics + series).
- Deterministic DBN→1m bars pipeline (NASDAQ calendar, America/New_York).
  - Warm ingest (cached DBN) per symbol‑year ≤ 10 min; cold ingest bound by network.
- E2E backtest on baseline (AAPL, 2023, sma_crossover fast=20, slow=50) completes ≤ 30s on Apple M2‑class dev laptop.

### User Success Metrics (owner‑operator)
- Config→run→review loop (with cached data) ≤ 2 min for baseline run.
- Reproducibility:
  - Artifacts (equity.parquet, orders.parquet, fills.parquet, metrics.json) produce identical SHA‑256 when manifest unchanged.
  - Aggregate metrics tolerance only if float nondeterminism unavoidable: |Δ| ≤ 0.05% per metric.
- Productivity: ≥ 20 baseline backtests/hour on cached data (single‑symbol, 1y).

### KPIs (operational)
- Playback: avg FPS ≈ 30; p95 dropped‑frame ratio ≤ 5%; p99 frame latency ≤ 200 ms.
- Stream: startup ≤ 300 ms; p95 per‑frame server→client latency ≤ 120 ms.
- Ingestion: success rate ≥ 99%; retryable error rate ≤ 1%.
- Storage (derived only, excludes raw DBN):
  - 1m bars + TBBO aggregates per symbol‑year ≤ 250 MB.
  - Per‑run artifacts total ≤ 50 MB (metrics + equity + orders/fills).
- Catalog/API: list runs p95 latency ≤ 150 ms (100 runs); filter by symbol/date p95 ≤ 200 ms.

## MVP Scope (Draft 0)
### Core Features (Must Have)
- Data ingestion: `make data SYMBOL=AAPL YEAR=2023` pulls Databento DBN (TRADES+TBBO), derives 1m OHLCV + minute TBBO aggregates under NASDAQ calendar (TZ=America/New_York), writes Parquet + manifest.
- Backtest execution: Nautilus adapter consumes 1m bars; baseline strategy `sma_crossover {fast:int, slow:int}` with `seed`; simple slippage/fees (mid ± k·spread_mean + bps). Artifacts: metrics.json, equity.parquet, orders.parquet, fills.parquet, run-manifest.json (params, seed, code hash, dataset ids, calendar/TZ).
- API: POST /backtests (async create), GET /backtests (list/filter), GET /backtests/{id}, GET /backtests/{id}/stream?speed=60 (SSE frames with server-side decimation).
- Playback UI: Vite/TS + TradingView Lightweight Charts; play/pause/seek/speed; overlays for orders/fills; equity curve + key metrics.
- Persistence/catalog: runs under data/backtests/{run_id}/...; lightweight runs catalog (JSON/SQLite) for list/filter/search.
- Dev workflow: `make setup` (env), `make start` (backend+frontend), `make data`, `make backtest`; README quickstart covers baseline run.
- Determinism: manifests include calendar version, interval, TZ, corp-action source/version, strategy+params+seed, git hash, env lock; re-runs reproduce artifacts.

### Out of Scope (MVP)
- Paper trading, live trading, brokerage integration
- Multi-symbol/portfolio runs (API/manifest future-ready, enforced single-symbol now)
- Tick/1s simulation; order-book microstructure; latency models
- Advanced risk management; multi-asset (FX/crypto) execution
- Strategy library beyond baseline SMA crossover
- Auth/RBAC, multi-user, cloud deployment/CI/CD
- Extensive indicator library, alerts/notifications

### MVP Success Criteria
- Data: warm ingest per symbol-year ≤ 10 min; derived storage ≤ 250 MB/symbol-year; complete manifest
- Backtest: baseline run ≤ 30s on M2-class; artifacts identical hashes on re-run; metrics tolerance ≤ 0.05%
- Playback: ~1y → ~60s; avg FPS ≈ 30; p95 dropped-frame ≤ 5%; responsive controls (pause/seek/speed)
- API/UI: can create/list/view/stream a run; catalog list p95 ≤ 150 ms (100 runs)
- Dev UX: `make start` works from clean checkout; `make data`/`make backtest` succeed; quickstart reproduces baseline

## Post-MVP Vision (Draft 0)
### Phase 2 Features
- Multi-symbol/portfolio backtests (synchronized playback; per-symbol + portfolio metrics)
- Parameter sweeps and grid/random search with results matrix and comparison tools
- Paper trading (simulated broker): streaming market feed, order ack/fill simulation, PnL attribution
- Enhanced playback: zoom-driven fidelity (minute→second where available), per-trade drilldown
- Strategy library expansion: additional baselines (e.g., breakout, mean reversion), indicator utilities

### Long-term Vision
- Live trading mode with broker adapters (abstract interface; concrete adapters added later)
- Robust risk management (position sizing, exposure/limits, kill-switches), alerting/monitoring
- Multi-market expansion (FX/crypto) behind clean data/clock abstractions
- Scalable catalogs: cloud/off-box storage for artifacts; optional batch backtest runners

### Expansion Opportunities
- Distributed backtest orchestration for large sweeps
- Research notebooks integration for quick analysis on artifacts (metrics/equity/orders)
- Optional data enrichments (corporate actions, fundamentals) applied in analytics layer

### Agent Orchestration (Research/Roadmap)
- Vision: Evolve from a single StrategyRunner to orchestrated agents (data, strategy search, execution, evaluation) coordinated by an orchestrator with memory and policies.
- Framework candidates: evaluate graph-based orchestration (e.g., LangGraph) versus custom lightweight orchestrator; keep vendor/framework agnostic interfaces.
- Tooling surface: agents operate over first‑class tools (Databento ingestion, bar generation, Nautilus backtest, runs catalog, metrics/equity inspectors).
- Safety/guardrails: policy constraints, budget/time limits, kill‑switches; pre‑deployment checklists; sandboxed dry‑run mode.
- Observability: structured logs, traces, run manifests; evaluation harness for agent actions and outcomes.
- Interfaces: agents read/write manifests and propose parameterized runs; orchestrator manages sweeps and comparisons; results persisted in catalog.
- Post‑MVP stance: document design and run small prototypes; do not block core backtesting loop or UI on agent features.


## Technical Considerations (Draft 0)
### Platform Requirements
- Target platforms: macOS (Apple Silicon) for dev; Linux (x86_64/ARM64) optional for headless jobs
- Runtime: Python 3.11+; Node 20+; modern Chrome/Safari (latest 2 versions)
- Performance: ~1y→~60s playback at ~30 FPS; end‑to‑end baseline backtest ≤ 30s (cached data)
- Time/Calendar: NASDAQ market calendar pinned; TZ=America/New_York; explicit DST handling

### Technology Preferences
- Backend: FastAPI + Uvicorn; Pydantic v2; async‑first design; Typer CLI for jobs (ingest/backtest)
- Data/Compute: Polars or Pandas; PyArrow/Parquet; deterministic rebar; simple slippage/fees model
- Market Data: Databento Python SDK (DBN TRADES+TBBO) → deterministic 1m bars + TBBO aggregates
- Backtesting: Nautilus Trader with a custom adapter for 1m bar feed + slippage hooks
- Storage: Local filesystem; Parquet for bars/series; JSON for manifests/metrics; SQLite/JSON catalog
- Frontend: Vite + React + TypeScript + Tailwind CSS v4; TradingView Lightweight Charts; SSE client + Web Worker for stream parsing
  - Component philosophy: dumb/presentational components only; no data computation/creation/mutation in UI; containers/hooks/services provide ready‑to‑render data
  - Data layer: TanStack Query for fetch/cache; Zod validation at boundaries; SSE decoded in Web Worker; formatting-only in components (no business logic)
- Tooling: Makefile; uv/poetry for env; Ruff/Black; pre‑commit; mypy optional; Vitest/ESLint/Prettier on FE

### Architecture Considerations
- Repository structure (proposed):
  - backend/app (FastAPI), backend/api (routes/schemas), backend/jobs (ingest/backtest), backend/adapters (databento→bars, bars→nautilus)
  - frontend/src (charts, views, api client, workers)
  - data/raw/databento, data/derived/bars, data/backtests (artifacts by run_id)
  - Frontend structure:
    - frontend/src/components/presentational (pure UI; props in, no side effects)
    - frontend/src/containers (compose data + presentational components)
    - frontend/src/services (API client, query keys, adapters, schema validation)
    - frontend/src/workers (SSE/stream parsing)
    - frontend/src/features/backtests (views: RunsList, RunDetail)
  - UI contract: presentational components never perform network calls or mutate data; they only format and display props

  - scripts/ (helper CLIs), Makefile, .env.example
- Integration requirements: DATABENTO_API_KEY from env; config via .env + pydantic‑settings; never commit secrets
- Streaming: SSE one‑way frames with server‑side decimation; adaptive frame skip; client backpressure handling
- Manifest schema (minimum):
  - run_id, created_at, symbol(s), from, to, interval, calendar_version, tz
  - strategy_id, params, seed, slippage/fees, code_hash, env_lock
  - dataset_ids (trades_dbn, tbbo_dbn, bars_parquet, bars_manifest)
- Multi‑symbol readiness: API/manifest accept arrays but enforce len=1 in MVP
- Testing: fixtures for DBN→1m bars; adapter conformance tests; deterministic seed tests; end‑to‑end smoke with baseline


## Constraints & Assumptions (Draft 0)
### Constraints
- Single user (owner‑operator); no auth/RBAC in MVP
- Equities only; NASDAQ calendar pinned; TZ=America/New_York; explicit DST handling
- Data: Databento TRADES + TBBO (DBN) locally cached; derived 1m bars + minute TBBO aggregates (Parquet)
- Backtests: single symbol per run (MVP); interval fixed to 1m; simple slippage/fees model
- Streaming: SSE only (one‑way); server‑side decimation to ~30 FPS
- Storage: local filesystem; Parquet/JSON artifacts; lightweight JSON/SQLite catalogs
- Frontend: Vite + React + TS + Tailwind 4; “dumb/presentational” UI components only
- Developer workflow: Makefile targets (`make setup|start|data|backtest`); Python 3.11+, Node 20+

### Key Assumptions
- Databento entitlements and sufficient historical coverage for selected symbols/years
- Nautilus Trader can consume our 1m bar feed via adapter with acceptable performance
- 1m bars are adequate fidelity for MVP validation; intra‑minute effects handled by slippage/fees approximation
- Apple M2‑class laptop is the baseline dev environment for performance targets
- Reproducibility achievable via manifests (params/seed/code hash/dataset ids/calendar version) and artifact hashing
- Playback target (~1y → ~60s) acceptable with frame skipping/decimation and minimal UI jank

## Target Users (Draft 0)
### Primary User Segment: Owner‑Operator (You)
- Profile: Single user; technical; comfortable with CLI/Make/venv; prefers deterministic, scriptable workflows
- Behaviors/Workflow: Runs many backtests across years/symbols; reviews equity/metrics; re‑runs with tweaks; curates a catalog of results
- Needs/Pain Points: Fast iteration; trustworthy, reproducible runs; minimal setup; no UI “magic” changing data; ability to replay results quickly
- Goals: Validate ideas rapidly; compare runs apples‑to‑apples; build path to paper/live with confidence
- Environment: Apple Silicon laptop; local data cache; intermittent focus blocks; prefers dumb UI + strong backend
- Decision Criteria: Speed (setup→insight), determinism, clear artifacts, simple ops (make start/data/backtest)

### Secondary User Segment (Future): Collaborator/Reviewer (technical)
- Profile: Occasional partner who reviews runs/metrics; may propose parameter sweeps
- Needs: Browse/search catalog; compare runs; minimal local setup; shareable manifests

## Risks & Open Questions (Draft 0)
### Key Risks
- Databento ingest: holidays/partial sessions/DST; DBN schema drift; gaps/corruption; entitlements/costs
- Bar fidelity (1m): intra‑minute path loss may bias fills; extreme spikes; corporate‑action handling
- Nautilus adapter: schema/time alignment mismatches; performance bottlenecks
- Reproducibility: missing metadata (seed/code hash/dataset ids); environment drift
- Storage/scale: multi‑year × many symbols footprint; catalog/list latency; retention
- Streaming/Playback: SSE throughput, client jank; frame drops at high compression
- Frontend perf: Lightweight Charts limits with large updates/overlays
- Security/ops: secrets handling, error budgets, failure modes for long runs

### Open Questions
- Symbol universe (baseline): which symbols first? how many years by default?
- Exact bar schema: columns (OHLCV, vwap, bid_mean, ask_mean, spread_mean, trades_count?)
- Slippage/fees defaults: k for spread, bps fees; per‑symbol overrides?
- Metrics set: which are mandatory in MVP vs later? definitions/tolerances?
- API details: pagination, filters, run_id scheme, idempotency for POST /backtests
- Catalog: JSON vs SQLite (or both); indexing strategy; retention policy
- Streaming transport: any reason to prefer WebSocket in MVP?
- Baseline strategy params: sma fast/slow defaults; seed policy
- Corporate actions: unadjusted vs adjusted usage in analytics; data source

### Areas Needing Further Research
- Databento DBN best practices for TRADES+TBBO; reliable NASDAQ calendar library
- Nautilus integration patterns and adapter examples
- TradingView Lightweight Charts update patterns at high rates
- Polars vs Pandas performance for rebar/aggregations on M1
- LangGraph and other orchestration frameworks (post‑MVP)


## Appendices (Draft 0)
### A. Research Summary
- Placeholder: Summaries of market data ingestion (Databento DBN best practices), Nautilus adapter design notes, Lightweight Charts perf findings, Polars vs Pandas benchmarks

### B. Stakeholder Input
- Placeholder: Notes from owner‑operator reviews and decisions (symbols, years, metrics set, bar schema, slippage defaults)

### C. References
- Databento docs (DBN/TRADES/TBBO)
- Nautilus Trader docs/examples
- Lightweight Charts docs
- NASDAQ market calendar library
- Polars/Pandas/Arrow/Parquet docs

## Next Steps
### Immediate Actions
1. Lock baseline: symbol(s) for MVP (e.g., AAPL) and years (e.g., 2023)
2. Finalize bar schema columns and manifest JSON schema
3. Define slippage/fees defaults (k, bps) and baseline strategy params (fast/slow)
4. Draft API contracts (POST/GET /backtests, SSE stream payloads) and run_id scheme
5. Choose catalog format (JSON vs SQLite) and indexing
6. Write Makefile target specs (`setup`, `start`, `data`, `backtest`) and README quickstart outline

### PM Handoff
This Project Brief provides the context and specifications for Hewston’s backtesting MVP. The next step is to start “PRD Generation Mode” and collaborate section‑by‑section to turn this into implementation tickets and acceptance criteria.