# Story 8.3: Chart Data Aggregation Endpoint

**Epic**: Epic 8 â€” Backend-for-Frontend (BFF) Implementation
**Priority**: High
**Effort**: 3-4 days
**Dependencies**: Story 8.2 (Basic HTTP Proxy Implementation)
**Status**: Ready for Development

## User Story

As a **trader**,  
I want **charts to load faster with consistent data formats across all timeframes**,  
So that **I can analyze market data efficiently without multiple loading states and data transformation complexity**.

## Story Context

### Existing System Integration

- **Integrates with**: Existing backend bar endpoints (`/bars/daily`, `/bars/minute`, `/bars/hour`)
- **Technology**: FastAPI async patterns, Pydantic data models, optional Redis caching
- **Follows pattern**: Existing backend data transformation and response formatting
- **Touch points**: Frontend chart components, backend bar data APIs, data decimation logic

### Business Context

This story addresses a key pain point where frontend components must make multiple API calls and perform complex data transformations for chart rendering. By aggregating and optimizing data at the BFF layer, we reduce frontend complexity and improve chart loading performance.

## Acceptance Criteria

### Functional Requirements

1. **Unified Chart Data Endpoint**
   - New endpoint `/api/v1/chart-data` accepts unified parameters for all timeframes
   - Single endpoint replaces multiple calls to `/bars/daily`, `/bars/minute`, `/bars/hour`
   - Timeframe parameter supports: "1D", "1H", "1M", "1M_DECIMATED"
   - Date range filtering with `from_date` and `to_date` parameters

2. **Data Aggregation Logic**
   - BFF determines appropriate backend endpoint based on timeframe parameter
   - Multiple backend calls aggregated into single response when needed
   - Data transformation from backend format to frontend-optimized format
   - Consistent OHLCV data structure across all timeframes

3. **Data Decimation Implementation**
   - Move data decimation logic from frontend to BFF layer
   - `target_points` parameter controls data reduction (default: 10000 points)
   - Intelligent decimation preserves important price movements
   - Metadata indicates when decimation was applied and stride used

4. **Response Optimization**
   - Standardized response format with bars array and metadata
   - Metadata includes total bars, decimation status, cache hit information
   - Response timing information for performance monitoring
   - Consistent error handling for invalid symbols or date ranges

### Integration Requirements

5. **Backend Compatibility**
   - Existing backend bar endpoints continue to work unchanged
   - No modifications to backend data formats or API contracts
   - BFF handles all backend communication and error scenarios
   - Backend authentication and authorization preserved

6. **Frontend Integration**
   - Response format designed for easy frontend chart component consumption
   - Existing chart data transformation logic can be removed from frontend
   - Compatible with existing chart libraries and rendering patterns
   - Error responses provide actionable information for frontend handling

7. **Performance Requirements**
   - Chart data loading completes within 2 seconds for any symbol/timeframe
   - Caching reduces repeated requests for same data
   - Async processing handles multiple concurrent chart requests
   - Memory efficient handling of large datasets

### Quality Requirements

8. **Caching Strategy**
   - Redis caching for frequently requested chart data
   - Cache keys based on symbol, timeframe, and date range
   - TTL-based cache expiration (5 minutes for recent data, longer for historical)
   - Cache hit/miss metrics for performance monitoring

9. **Data Validation**
   - Input validation for symbol format, date ranges, and timeframe values
   - Backend data validation and error handling
   - Graceful handling of missing or incomplete data
   - Data consistency checks across timeframes

10. **Testing Coverage**
    - Unit tests for data transformation and decimation logic
    - Integration tests with mock backend data
    - Performance tests with large datasets
    - Cache behavior testing and invalidation scenarios

## Technical Notes

### Data Transformation Logic
```python
# Example transformation from backend to frontend format
def transform_bars_for_frontend(backend_bars: List[BackendBar], timeframe: str) -> List[FrontendBar]:
    return [
        FrontendBar(
            timestamp=bar.t.isoformat(),
            open=bar.o,
            high=bar.h,
            low=bar.l,
            close=bar.c,
            volume=bar.v or 0
        )
        for bar in backend_bars
    ]
```

### Decimation Algorithm
- Use time-based sampling for consistent intervals
- Preserve high/low extremes within each sample window
- Maintain volume accuracy through aggregation
- Include metadata about decimation parameters

### Caching Strategy
- Cache key format: `chart:{symbol}:{timeframe}:{from_date}:{to_date}:{target_points}`
- TTL based on data recency: 5min for current day, 1hr for recent, 24hr for historical
- Cache warming for popular symbols and timeframes
- Graceful degradation when cache unavailable

### Integration Approach
- Extend existing BFF proxy infrastructure from Story 8.2
- Add data transformation service layer
- Implement caching service with Redis integration
- Follow existing FastAPI async patterns

## Definition of Done

- [ ] **Unified Endpoint**: `/api/v1/chart-data` successfully aggregates data from multiple backend endpoints
- [ ] **Data Transformation**: Backend bar data transformed to consistent frontend format
- [ ] **Decimation Logic**: Data decimation implemented and moved from frontend to BFF
- [ ] **Caching**: Redis caching implemented with appropriate TTL and key strategies
- [ ] **Performance**: Chart data loads within 2 seconds for all supported scenarios
- [ ] **Testing**: Comprehensive tests cover transformation, decimation, and caching logic
- [ ] **Documentation**: API documentation and frontend integration guide created
- [ ] **Monitoring**: Metrics collection for cache hit rates and response times

## Risk Assessment

### Primary Risk
**Data Consistency**: Aggregated data doesn't match direct backend calls

### Mitigation
- Comprehensive data validation and transformation testing
- Side-by-side comparison testing with existing frontend logic
- Contract testing to ensure data format consistency

### Secondary Risk
**Performance Degradation**: Data processing in BFF impacts response times

### Mitigation
- Async processing for backend calls
- Intelligent caching strategy
- Performance monitoring and alerting

### Rollback Plan
- Feature flag to disable aggregated endpoint
- Frontend can fall back to direct backend calls
- Cache can be bypassed for troubleshooting

## Testing Strategy

### Unit Tests
- Data transformation logic for all timeframes
- Decimation algorithm accuracy and performance
- Cache key generation and TTL logic
- Input validation and error handling

### Integration Tests
- End-to-end chart data flow with mock backend
- Cache behavior with Redis integration
- Error scenarios (backend failures, invalid data)
- Performance testing with large datasets

### Contract Tests
- Response format validation against frontend expectations
- Data consistency between aggregated and direct backend calls
- Error response format compatibility

### Performance Tests
- Response time measurement for various data sizes
- Concurrent request handling
- Cache performance and hit rate optimization
- Memory usage with large datasets

---

**Story ID**: S8.3
**Created**: 2025-01-27
**Epic Reference**: [`docs/prd/epic-8-bff-implementation.md`](../prd/epic-8-bff-implementation.md)
**Architecture Reference**: [`docs/architecture/bff-architecture.md`](../architecture/bff-architecture.md)
