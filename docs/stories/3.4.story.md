# Story 3.4 — Ingest rate limits and retry policy (Databento)
Story ID: S3.4
Epic ID: E3

Status: Draft

User Story
- As a data engineer,
- I want the ingest job to respect Databento rate limits and implement a clear retry/backoff policy,
- So that ingestion is reliable and doesn’t fail flakily or trigger provider throttling.

Context
- Applies to jobs/ingest.py and any client wrappers. Local-first; simple, robust defaults.

Dependencies
- Depends on: S3.1 (Databento ingestion CLI)

Definition of Ready
- Provider limits understood; SDK behavior noted (concurrency, Retry-After)
- Baseline dataset defined (AAPL 2023) for verification
- Error codes aligned; logging fields decided

- Blocks: None (quality improvement within E3)

References
- Baselines (dataset, calendar/TZ): docs/prd/features/00-baselines.md
- Error codes (for surfaced failures): docs/api/error-codes.md

Acceptance Criteria
1) Policy defined and implemented:
   - Max concurrent requests to provider: documented constant (or single-threaded if SDK-bound)
   - Exponential backoff with jitter on HTTP 429/5xx, finite retries with clear stop condition
   - Respect provider retry-after headers when present
2) Logging & visibility:
   - Structured logs include attempt number, backoff duration, and terminal outcome
   - Summary log: total bytes/files fetched, elapsed time
3) Failure semantics:
   - Idempotent retries; partial files not left in inconsistent state (write temp then move)
   - On terminal failure: return non-zero exit and structured error code (CONFLICT/BUSY/INTERNAL as appropriate)
4) Documentation:
   - Note the policy in a short comment block and/or docs/data/databento-ingest-notes.md (optional)

Definition of Done
- Ingest job does not rapidly retry on provider throttle; logs show controlled retries; failures are structured and actionable.

