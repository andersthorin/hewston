# Story 4.2 — Run job + write artifacts
Story ID: S4.2
Epic ID: E4



Status: Draft

User Story
- As a platform engineer,
- I want a Typer job that executes a backtest using the Nautilus adapter and writes artifacts to disk with a run manifest,
- So that completed runs are persisted and discoverable by the catalog and playback services.

Context
- Second story of Epic 4. Persists outputs from 4.1 to the filesystem and updates the catalog.

Acceptance Criteria
1) Implement `backend/jobs/run_backtest.py` and expose via `backend/jobs/cli.py backtest` command
2) Artifacts written under `data/backtests/{run_id}/`:
   - metrics.json, equity.parquet, orders.parquet, fills.parquet, run-manifest.json
3) run-manifest.json includes: run_id, dataset_id, strategy_id, params, seed, slippage_fees, speed, code_hash, env_lock, calendar_version, tz, created_at
4) Catalog updates: insert `runs` row (QUEUED→RUNNING→DONE/ERROR) and `run_metrics` row (minimal metrics)
5) CLI prints run_id and exit code 0 on success; non-zero on failure; logs show duration_ms
6) Idempotent file layout and stable schema names; lint passes

Dev Notes (sourced from architecture docs)
- Artifacts layout and fields [Source: architecture.md#data-models]
- Catalog DDL for runs and run_metrics [Source: architecture.md#catalog-schema-sqlite-ddl]
- Implementation Plan Milestone 5 [Source: architecture.md#implementation-plan--roadmap-mvp]

Technical Specifications
- Command: `backtest --dataset-id ... --strategy-id ... --param fast=20 --param slow=50 --seed ... --speed 60`
- Artifact writing: Parquet via PyArrow/Polars; JSON via stdlib; timestamps ISO-8601 UTC
- Catalog transitions: create row (QUEUED), set RUNNING, finalize to DONE with duration_ms and metrics
- Manifest: include hashes available (code_hash via git rev-parse if available; env_lock placeholder OK for MVP)

Tasks / Subtasks
- Implement CLI parsing in jobs/cli.py → calls jobs/run_backtest.py
- Call `BacktestRunnerPort.run` and collect results
- Write artifacts to `data/backtests/{run_id}/` and compute basic metrics JSON
- Upsert run + metrics into SQLite via CatalogPort
- Print run_id and duration; return exit status accordingly
- `make lint` backend/

Testing & Validation
- With a dataset present, run: `make backtest SYMBOL=AAPL FROM=2023-01-01 TO=2023-12-31 FAST=20 SLOW=50`
- Verify artifacts exist and non-empty; view metrics.json; query SQLite for runs + run_metrics rows

Definition of Done
- Backtest job writes artifacts and updates catalog correctly with proper status transitions; manifests recorded; CLI exits cleanly.

